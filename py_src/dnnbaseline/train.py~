from ..tnt import torchnet as tnt
import torch
from ..utils import kaldi_io
import argparse
import os
from ..utils.kaldi_dataloader import KaldiStreamDataloader
import logging
from .DNN import DNN
from torch.autograd import Variable
import numpy as np
from tqdm import tqdm
from torch.optim.lr_scheduler import ReduceLROnPlateau

def save_checkpoint(state,is_best, savedir):
    filename = os.path.join(savedir, 'checkpoint.th')
    torch.save({'state_dict': state['model'].state_dict(),
                'epoch': state['epoch']}, filename)
    if is_best:
        torch.save(state['model'].state_dict(),
                   os.path.join(savedir, 'model_best.param'))


def load_mean_var(fpath):
    mv = np.loadtxt(fpath)
    return torch.FloatTensor(mv[0]), torch.FloatTensor(mv[1])


def get_logger():
    logging.basicConfig(level=logging.INFO,
                        format="[ %(levelname)s : %(asctime)s ] - %(message)s")
    logger = logging.getLogger("train")
    return logger


def parse_arg():
    parser = argparse.ArgumentParser()
    parser.add_argument("traindata", type=str)
    parser.add_argument("trainlabel", type=lambda x:{
        k: v for k, v in kaldi_io.read_ali_ark(x)
    })
    parser.add_argument("cvdata", type=str)
    parser.add_argument("cvlabel", type=lambda  x:{
        k: v for k, v in kaldi_io.read_ali_ark(x)
    })
    parser.add_argument("traincounts", type=argparse.FileType("r"))
    parser.add_argument("cvcounts", type=argparse.FileType("r"))
    parser.add_argument("outputdim", type=int)
    parser.add_argument("meanvar", type=argparse.FileType("r"))
    parser.add_argument("--nocuda", action="store_true", default=False)
    parser.add_argument("--nonorm", action="store_true", default=False)
    parser.add_argument("-ep", "--epochs", default=50, type=int)
    parser.add_argument("-lr", '--learningrate', default=1e-2, type=float)
    parser.add_argument("-bs", '--batchsize', default=256, type=int)
    parser.add_argument("-o", "--output", type=str, required=True)

    args = parser.parse_args()
    os.makedirs(args.output, exist_ok=True)
    return args


def main():

    args = parse_arg()
    logger = get_logger()
    traindataloader = KaldiStreamDataloader(
        args.traindata, args.trainlabel, countsfile=args.traincounts, num_outputs=args.outputdim,
        batchsize=args.batchsize, cachesize=1 * args.batchsize, shuffle=True)

    cvdataloader = KaldiStreamDataloader(
        args.cvdata, args.cvlabel, countsfile=args.cvcounts, num_outputs=args.outputdim,
        batchsize=args.batchsize, cachesize=1 * args.batchsize, shuffle=False)

    logger.info("train_nsample:"+str(traindataloader.nsamples))
    logger.info("cv_nsample:"+str(cvdataloader.nsamples))

    model = DNN(traindataloader.inputdim, args.outputdim)
    # devide_ids = [0, 1]
    # model = torch.nn.DataParallel(model, device_ids=devide_ids)
    print("CUDA_VISIBLE_DEVICES", str(os.environ['CUDA_VISIBLE_DEVICES']))
    if not args.nocuda:
        model = model.cuda()
    print(model)
    tr_mean, tr_var = load_mean_var(args.meanvar)

    epoch = 0
    epochsize = int(traindataloader.nsamples * 1.0 / args.batchsize)
    cvsize = int(cvdataloader.nsamples * 1. // args.batchsize)
    engine = tnt.engine.Engine()
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learningrate, weight_decay=1e-8)
    sched = ReduceLROnPlateau(optimizer,mode="max", factor=0.5, patience=0)

    # Statistics
    time_meter = tnt.meter.TimeMeter(False)
    meter_acc = tnt.meter.AverageValueMeter()
    meter_loss = tnt.meter.AverageValueMeter()

    def lossfunction(sample):
        x, y = sample
        if not args.nonorm:
            x = torch.add(x, -tr_mean)
            x = torch.div(x, tr_var)
        if not args.nocuda:
            x, y = x.cuda(), y.cuda()
        x, y = Variable(x), Variable(y)
        outputs = model(x)
        loss = criterion(outputs, y)
        return loss, outputs

    def evalfunction(sample):
        x, y = sample
        if not args.nonorm:
            x = torch.add(x, -tr_mean)
            x = torch.div(x, tr_var)
        if not args.nocuda:
            x, y = x.cuda(), y.cuda()
        x_var, y_var = Variable(x, volatile=True), Variable(y, volatile=True)
        outputs = model(x_var)
        loss = criterion(outputs,y_var)
        _, predicted = torch.max(outputs.data, 1)
        acc = (predicted == y).sum() * 1. / len(y)
        return {'loss': loss, 'acc': acc}, outputs

    def reset_meters():
        meter_acc.reset()
        meter_loss.reset()

    def on_start(state):
        state['best_acc'] = state[
            'best_acc'] if 'best_acc' in state else 0.

    def on_end_epoch(state):
        # Output trainmessge
        trainmessage = 'Training Epoch {:>3d}: Time: {:=6.1f}s/{:=4.1f}m Loss: {:=.4f} LR: {:=3.1e}'.format(
            state['epoch'], time_meter.value(), time_meter.value()/60, meter_loss.value()[0],optimizer.param_groups[0]['lr'])
        print(trainmessage)

        # evaluate procedure
        reset_meters()
        model.eval()
        engine.hooks['on_forward'] = on_forward_test
        engine.test(evalfunction, tqdm(cvdataloader, total=cvsize))
        engine.hooks['on_forward'] = on_forward
        acc = meter_acc.value()[0]
        loss = meter_loss.value()[0]
        evalmessage = 'CV Epoch {:>3d}: Time: {:=.2f}s/{:=.2f}m, acc:{:=.4f}, loss:{:=.8f}'.format(
            state['epoch'], time_meter.value(), time_meter.value()/60, acc, loss)
        print(evalmessage)
        sched.step(acc)

        # save model
        isbest = acc > state['best_acc']
        state['best_acc'] = acc if isbest else state['best_acc']
        save_checkpoint({
            'model': model,
            'epoch': state['epoch']
        }, isbest, args.output
        )

        # Quit training if learning rate is below 1e-9
        if optimizer.param_groups[0]['lr'] < 1e-9:
            state['epoch'] = 1e30
            print('lr<1e-9 stop training')
            return

    def on_start_epoch(state):
        model.train()
        reset_meters()
        state['iterator'] = tqdm(
            state['iterator'], total=epochsize, unit="batch", leave=False)

    def on_forward(state):
        meter_loss.add(state['loss'].data[0])

    def on_forward_test(state):
        meter_acc.add(state['loss']['acc'])
        meter_loss.add(state['loss']['loss'].data[0])

    engine.hooks['on_start'] = on_start
    engine.hooks['on_forward'] = on_forward
    engine.hooks['on_start_epoch'] = on_start_epoch
    engine.hooks['on_end_epoch'] = on_end_epoch

    engine.train(lossfunction, traindataloader,
                 maxepoch=args.epochs, optimizer=optimizer, epoch=epoch)



if __name__ == '__main__':
    main()
